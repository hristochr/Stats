{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bayes Theorem",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwy7wXfu2fVJ",
        "colab_type": "text"
      },
      "source": [
        "#Bayes Theorem of Conditional Probability\n",
        "\n",
        "[source](https://machinelearningmastery.com/bayes-theorem-for-machine-learning/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oKeZTzx2cPv",
        "colab_type": "text"
      },
      "source": [
        "Let’s review marginal, joint, and conditional probability.\n",
        "\n",
        "Recall that marginal probability is the probability of an event, irrespective of other random variables. If the random variable is independent, then it is the probability of the event directly, otherwise, if the variable is dependent upon other variables, then the marginal probability is the probability of the event summed over all outcomes for the dependent variables, called the sum rule.\n",
        "\n",
        "- Marginal Probability: The probability of an event irrespective of the outcomes of other random variables, e.g. P(A).\n",
        "- The joint probability is the probability of two (or more) simultaneous events, often described in terms of events A and B from two dependent random variables, e.g. X and Y. The joint probability is often summarized as just the outcomes, e.g. A and B.\n",
        "Joint Probability: Probability of two (or more) simultaneous events, e.g. P(A and B) or P(A, B).\n",
        "- The conditional probability is the probability of one event given the occurrence of another event, often described in terms of events A and B from two dependent random variables e.g. X and Y. \n",
        "Conditional Probability: Probability of one (or more) event given the occurrence of another event, e.g. P(A given B) or P(A | B).\n",
        "\n",
        "The joint probability can be calculated using the conditional probability; for example:\n",
        "\n",
        "$$P(A, B) = P(A | B) * P(B)$$\n",
        "\n",
        "This is called the product rule. Importantly, the joint probability is symmetrical, meaning that:\n",
        "\n",
        "$$P(A, B) = P(B, A)$$\n",
        "The conditional probability can be calculated using the joint probability; for example:\n",
        "\n",
        "$$P(A | B) = \\frac{P(A, B)}{P(B)}$$\n",
        "The conditional probability is not symmetrical; for example:\n",
        "\n",
        "$$P(A | B) != P(B | A)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvABx9d83LQx",
        "colab_type": "text"
      },
      "source": [
        "## An Alternate Way To Calculate Conditional Probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFkMw9TM3Lfu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now, there is another way to calculate the conditional probability.\n",
        "\n",
        "Specifically, one conditional probability can be calculated using the other conditional probability; for example:\n",
        "\n",
        "$$P(A|B) = P(B|A) * \\frac{P(A)}{P(B)}$$\n",
        "The reverse is also true; for example:\n",
        "\n",
        "$$P(B|A) = P(A|B) * \\frac{P(B)}{P(A)}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR3U-lvj3blj",
        "colab_type": "text"
      },
      "source": [
        "*This alternate calculation of the conditional probability is referred to as Bayes Rule or Bayes Theorem, named for Reverend Thomas Bayes, who is credited with first describing it. It is grammatically correct to refer to it as Bayes’ Theorem (with the apostrophe), but it is common to omit the apostrophe for simplicity.*\n",
        "\n",
        "**Bayes Theorem: Principled way of calculating a conditional probability without the joint probability.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzK83SBi3vaN",
        "colab_type": "text"
      },
      "source": [
        "It is often the case that we do not have access to the denominator directly, e.g. $P(B)$.\n",
        "\n",
        "We can calculate it an alternative way; for example:\n",
        "\n",
        "$$P(B) = P(B|A) * P(A) + P(B|\\neg A) * P(\\neg A)$$\n",
        "This gives a formulation of Bayes Theorem that we can use that uses the alternate calculation of P(B), described below:\n",
        "\n",
        "$$P(A|B) = P(B|A) * \\frac{P(A)}{P(B|A)} * P(A) + P(B|\\neg A) * P(\\neg A)$$\n",
        "Or with brackets around the denominator for clarity:\n",
        "\n",
        "$$P(A|B) = P(B|A) * \\frac{P(A)}{P(B|A) * P(A) + P(B|\\neg A) * P(\\neg A)}$$\n",
        "Note: the denominator is simply the expansion we gave above.\n",
        "\n",
        "If we have $P(A)$, then we can calculate $P(\\neg A)$ as its complement; for example:\n",
        "\n",
        "$$P(\\neg A) = 1 – P(A)$$\n",
        "Additionally, if we have $P(\\neg B|\\neg A)$, then we can calculate $P(B|\\neg A)$ as its complement; for example:\n",
        "\n",
        "$$P(B|\\neg A) = 1 – P(\\neg B|\\neg A)$$\n",
        "Now that we are familiar with the calculation of Bayes Theorem, let’s take a closer look at the meaning of the terms in the equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te_3BTAL4vqO",
        "colab_type": "text"
      },
      "source": [
        "## Naming the Terms in the Theorem\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qNTFm5j4td9",
        "colab_type": "text"
      },
      "source": [
        "Firstly, in general, the result $P(A|B$) is referred to as the posterior probability and $P(A)$ is referred to as the prior probability.\n",
        "\n",
        "- $P(A|B)$: Posterior probability.\n",
        "- $P(A)$: Prior probability.\n",
        "\n",
        "Sometimes $P(B|A)$ is referred to as the likelihood and $P(B)$ is referred to as the evidence.\n",
        "\n",
        "- $P(B|A)$: Likelihood.\n",
        "- $P(B)$: Evidence.\n",
        "\n",
        "This allows Bayes Theorem to be restated as:\n",
        "\n",
        "$$Posterior = Likelihood * \\frac{Prior}{Evidence}$$\n",
        "\n",
        "Example:\n",
        "\n",
        "What is the probability that there is fire given that there is smoke?\n",
        "\n",
        "Where $P(Fire)$ is the Prior, $P(Smoke|Fire)$ is the Likelihood, and $P(Smoke)$ is the evidence:\n",
        "\n",
        "$$P(Fire|Smoke) = P(Smoke|Fire) * \\frac{P(Fire)}{P(Smoke)}$$\n",
        "\n",
        "You can imagine the same situation with rain and clouds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsepLmgG5xgX",
        "colab_type": "text"
      },
      "source": [
        "## Diagnostic test scenario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR2L_XEp5zyK",
        "colab_type": "text"
      },
      "source": [
        "Diagnostic Test Scenario\n",
        "An excellent and widely used example of the benefit of Bayes Theorem is in the analysis of a medical diagnostic test.\n",
        "\n",
        "Scenario: Consider a human population that may or may not have cancer (Cancer is True or False) and a medical test that returns positive or negative for detecting cancer (Test is Positive or Negative), e.g. like a mammogram for detecting breast cancer.\n",
        "\n",
        "Problem: If a randomly selected patient has the test and it comes back positive, what is the probability that the patient has cancer?\n",
        "\n",
        "**Manual Calculation**\n",
        "\n",
        "Medical diagnostic tests are not perfect; they have error.\n",
        "\n",
        "Sometimes a patient will have cancer, but the test will not detect it. This capability of the test to detect cancer is referred to as the sensitivity, or the true positive rate.\n",
        "\n",
        "In this case, we will contrive a sensitivity value for the test. The test is good, but not great, with a true positive rate or sensitivity of 85%. That is, of all the people who have cancer and are tested, 85% of them will get a positive result from the test.\n",
        "\n",
        "$$P(Test=Positive | Cancer=True) = 0.85$$\n",
        "Given this information, our intuition would suggest that there is an 85% probability that the patient has cancer.\n",
        "\n",
        "Our intuitions of probability are wrong.\n",
        "\n",
        "This type of error in interpreting probabilities is so common that it has its own name; it is referred to as the base rate fallacy.\n",
        "\n",
        "It has this name because the error in estimating the probability of an event is caused by ignoring the base rate. That is, it ignores the probability of a randomly selected person having cancer, regardless of the results of a diagnostic test.\n",
        "\n",
        "In this case, we can assume the probability of breast cancer is low, and use a contrived base rate value of one person in 5,000, or (0.0002) 0.02%.\n",
        "\n",
        "$$P(Cancer=True) = 0.02%$$\n",
        "We can correctly calculate the probability of a patient having cancer given a positive test result using Bayes Theorem.\n",
        "\n",
        "Let’s map our scenario onto the equation:\n",
        "\n",
        "$$P(A|B) = P(B|A) * \\frac{P(A)}{P(B)}$$\n",
        "\n",
        "$$P(Cancer=True | Test=Positive) = P(Test=Positive|Cancer=True) * \\frac{P(Cancer=True)}{P(Test=Positive)}$$\n",
        "\n",
        "We know the probability of the test being positive given that the patient has cancer is 85%, and we know the base rate or the prior probability of a given patient having cancer is 0.02%; we can plug these values in:\n",
        "\n",
        "$$P(Cancer=True | Test=Positive) = 0.85 * \\frac{0.0002}{P(Test=Positive)}$$\n",
        "\n",
        "We don’t know P(Test=Positive), it’s not given directly.\n",
        "\n",
        "Instead, we can estimate it using:\n",
        "\n",
        "$$P(B) = P(B|A) * P(A) + P(B|\\neg A) * P(\\neg A)$$\n",
        "\n",
        "$$P(Test=Positive) = P(Test=Positive|Cancer=True) * P(Cancer=True) + P(Test=Positive|Cancer=False) * P(Cancer=False)$$\n",
        "\n",
        "Firstly, we can calculate P(Cancer=False) as the complement of P(Cancer=True), which we already know\n",
        "\n",
        "$$P(Cancer=False) = 1 – P(Cancer=True)\n",
        "= 1 – 0.0002\n",
        "= 0.9998$$\n",
        "Let’s plugin what we have:\n",
        "\n",
        "We can plug in our known values as follows:\n",
        "\n",
        "$$P(Test=Positive) = 0.85 * 0.0002 + P(Test=Positive|Cancer=False) * 0.9998$$\n",
        "\n",
        "We still do not know the probability of a positive test result given no cancer.\n",
        "\n",
        "This requires additional information.\n",
        "\n",
        "Specifically, we need to know how good the test is at correctly identifying people that do not have cancer. That is, testing negative result (Test=Negative) when the patient does not have cancer (Cancer=False), called the true negative rate or the specificity.\n",
        "\n",
        "We will use a contrived specificity value of 95%.\n",
        "\n",
        "$$P(Test=Negative | Cancer=False) = 0.95$$\n",
        "\n",
        "With this final piece of information, we can calculate the false positive or false alarm rate as the complement of the true negative rate.\n",
        "\n",
        "$$P(Test=Positive|Cancer=False) = 1 – P(Test=Negative | Cancer=False)\n",
        "= 1 – 0.95\n",
        "= 0.05$$\n",
        "\n",
        "We can plug this false alarm rate into our calculation of P(Test=Positive) as follows:\n",
        "\n",
        "$$P(Test=Positive) = 0.85 * 0.0002 + 0.05 * 0.9998$$\n",
        "\n",
        "$$P(Test=Positive) = 0.00017 + 0.04999$$\n",
        "\n",
        "$$P(Test=Positive) = 0.05016$$\n",
        "\n",
        "Excellent, so the probability of the test returning a positive result, regardless of whether the person has cancer or not is about 5%.\n",
        "\n",
        "We now have enough information to calculate Bayes Theorem and estimate the probability of a randomly selected person having cancer if they get a positive test result.\n",
        "\n",
        "$$P(Cancer=True | Test=Positive) = P(Test=Positive|Cancer=True) * \\frac{P(Cancer=True)}{P(Test=Positive)}$$\n",
        "$$P(Cancer=True | Test=Positive) = 0.85 * 0.0002 / 0.05016$$\n",
        "$$P(Cancer=True | Test=Positive) = 0.00017 / 0.05016$$\n",
        "$$P(Cancer=True | Test=Positive) = 0.003389154704944$$\n",
        "\n",
        "The calculation suggests that if the patient is informed they have cancer with this test, then there is only 0.33% chance that they have cancer.\n",
        "\n",
        "It is a terrible diagnostic test!\n",
        "\n",
        "The example also shows that the calculation of the conditional probability requires enough information.\n",
        "\n",
        "For example, if we have the values used in Bayes Theorem already, we can use them directly.\n",
        "\n",
        "This is rarely the case, and we typically have to calculate the bits we need and plug them in, as we did in this case. In our scenario we were given 3 pieces of information, the the base rate, the  sensitivity (or true positive rate), and the specificity (or true negative rate).\n",
        "\n",
        "Sensitivity: 85% of people with cancer will get a positive test result.\n",
        "Base Rate: 0.02% of people have cancer.\n",
        "Specificity: 95% of people without cancer will get a negative test result.\n",
        "We did not have the P(Test=Positive), but we calculated it given what we already had available.\n",
        "\n",
        "We might imagine that Bayes Theorem allows us to be even more precise about a given scenario. For example, if we had more information about the patient (e.g. their age) and about the domain (e.g. cancer rates for age ranges), and in turn we could offer an even more accurate probability estimate.\n",
        "\n",
        "That was a lot of work.\n",
        "\n",
        "Let’s look at how we can calculate this exact scenario using a few lines of Python code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxeXKCyo7zHo",
        "colab_type": "text"
      },
      "source": [
        "## In python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8ScB7xi2abL",
        "colab_type": "code",
        "outputId": "c8b90ab0-be5e-45a5-9c73-21cfc297cfa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# calculate the probability of cancer patient and diagnostic test\n",
        "\n",
        "# calculate P(A|B) given P(A), P(B|A), P(B|not A)\n",
        "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
        "\t# calculate P(not A)\n",
        "\tnot_a = 1 - p_a\n",
        "\t# calculate P(B)\n",
        "\tp_b = p_b_given_a * p_a + p_b_given_not_a * not_a\n",
        "\t# calculate P(A|B)\n",
        "\tp_a_given_b = (p_b_given_a * p_a) / p_b\n",
        "\treturn p_a_given_b\n",
        "\n",
        "# P(A)\n",
        "p_a = 0.0002\n",
        "# P(B|A)\n",
        "p_b_given_a = 0.85\n",
        "# P(B|not A)\n",
        "p_b_given_not_a = 0.05\n",
        "# calculate P(A|B)\n",
        "result = bayes_theorem(p_a, p_b_given_a, p_b_given_not_a)\n",
        "# summarize\n",
        "print('P(A|B) = %.3f%%' % (result * 100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "P(A|B) = 0.339%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agi5pcqI74kN",
        "colab_type": "text"
      },
      "source": [
        "## Binary Classifier Terminology\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlT08-fF78TV",
        "colab_type": "text"
      },
      "source": [
        "  . |Positive Class| Negative Class\n",
        " ---|---|---\n",
        "Positive Prediction | True Positive (TP)  | False Positive (FP)\n",
        "Negative Prediction | False Negative (FN) | True Negative (TN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKyokEb-8aB5",
        "colab_type": "text"
      },
      "source": [
        "We can then define some rates from the confusion matrix:\n",
        "\n",
        "- True Positive Rate $(TPR) = \\frac{TP}{(TP + FN)}$\n",
        "- False Positive Rate $(FPR) = \\frac{FP}{(FP + TN)}$\n",
        "- True Negative Rate $(TNR) = \\frac{TN}{(TN + FP)}$\n",
        "- False Negative Rate $(FNR) = \\frac{FN}{(FN + TP)}$\n",
        "\n",
        "\n",
        "We can map these rates onto familiar terms from Bayes Theorem:\n",
        "\n",
        "- P(B|A): True Positive Rate (TPR).\n",
        "- P(not B|not A): True Negative Rate (TNR).\n",
        "- P(B|not A): False Positive Rate (FPR).\n",
        "- P(not B|A): False Negative Rate (FNR).\n",
        "\n",
        "\n",
        "We can also map the base rates for the condition (class) and the treatment (prediction) on familiar terms from Bayes Theorem:\n",
        "\n",
        "- P(A): Probability of a Positive Class (PC).\n",
        "- P(not A): Probability of a Negative Class (NC).\n",
        "- P(B): Probability of a Positive Prediction (PP).\n",
        "- P(not B): Probability of a Negative Prediction (NP).\n",
        "\n",
        "Now, let’s consider Bayes Theorem using these terms:\n",
        "\n",
        "$$P(A|B) = P(B|A) * \\frac{P(A)}{P(B)}$$\n",
        "$$P(A|B) = \\frac{(TPR * PC)}{PP}$$\n",
        "\n",
        "Where we often cannot calculate P(B), so we use an alternative:\n",
        "\n",
        "$$P(B) = P(B|A) * P(A) + P(B|\\neg A) * P(\\neg A)$$\n",
        "$$P(B) = TPR * PC + FPR * NC$$\n",
        "\n",
        "\n",
        "Now, let’s look at our scenario of cancer and a cancer detection test.\n",
        "\n",
        "The class or condition would be “Cancer” and the treatment or prediction would the “Test“.\n",
        "\n",
        "First, let’s review all of the rates:\n",
        "\n",
        "- True Positive Rate (TPR): 85%\n",
        "- False Positive Rate (FPR): 5%\n",
        "- True Negative Rate (TNR): 95%\n",
        "- False Negative Rate (FNR): 15%\n",
        "\n",
        "Let’s also review what we know about base rates:\n",
        "\n",
        "- Positive Class (PC): 0.02%\n",
        "- Negative Class (NC): 99.98%\n",
        "- Positive Prediction (PP): 5.016%\n",
        "- Negative Prediction (NP): 94.984%\n",
        "\n",
        "Plugging things in, we can calculate the probability of a positive test result (a positive prediction) as the probability of a positive test result given cancer (the true positive rate) multiplied by the base rate for having cancer (the positive class), plus the probability if a positive test result given no cancer (the false positive rate) plus the probability of not having cancer (the negative class).\n",
        "\n",
        "The calculation with these terms is as follows:\n",
        "\n",
        "$$P(B) = P(B|A) * P(A) + P(B|not A) * P(not A)$$\n",
        "$$P(B) = TPR * PC + FPR * NC$$\n",
        "$$P(B) = 85% * 0.02% + 5% * 99.98%$$\n",
        "$$P(B) = 5.016%$$\n",
        "\n",
        "We can then calculate Bayes Theorem for the scenario, namely the probability of cancer given a positive test result (the posterior) is the probability of a positive test result given cancer (the true positive rate) multiplied by the probability of having cancer (the positive class rate), divided by the probability of a positive test result (a positive prediction).\n",
        "\n",
        "The calculation with these terms is as follows:\n",
        "\n",
        "$$P(A|B) = P(B|A) * P(A) / P(B)$$\n",
        "$$P(A|B) = TPR * PC / PP$$\n",
        "$$P(A|B) = 85% * 0.02% / 5.016%$$\n",
        "$$P(A|B) = 0.339%$$\n",
        "\n",
        "It turns out that in this case, the posterior probability that we are calculating with the Bayes theorem is equivalent to the precision, also called the Positive Predictive Value (PPV) of the confusion matrix:\n",
        "\n",
        "$$PPV = \\frac{TP}{(TP + FP)}$$\n",
        "Or, stated in our classifier terms:\n",
        "\n",
        "$$P(A|B) = PPV$$\n",
        "$$PPV = TPR * PC / PP$$\n",
        "\n",
        "So why do we go to all of the trouble of calculating the posterior probability?\n",
        "\n",
        "Because we don’t have the confusion matrix for a population of people both with and without cancer that have been tested and have been not tested. Instead, all we have is some priors and probabilities about our population and our test.\n",
        "\n",
        "This highlights when we might choose to use the calculation in practice.\n",
        "\n",
        "Specifically, when we have beliefs about the events involved, but we cannot perform the calculation by counting examples in the real world."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTjjF5Ar-O0j",
        "colab_type": "text"
      },
      "source": [
        "## Bayes Theorem for Modeling Hypotheses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNSuyS2K-M_B",
        "colab_type": "text"
      },
      "source": [
        "Bayes Theorem is a useful tool in applied machine learning.\n",
        "\n",
        "It provides a way of thinking about the relationship between data and a model.\n",
        "\n",
        "A machine learning algorithm or model is a specific way of thinking about the structured relationships in the data. In this way, a model can be thought of as a hypothesis about the relationships in the data, such as the relationship between input (X) and output (y). The practice of applied machine learning is the testing and analysis of different hypotheses (models) on a given dataset.\n",
        "\n",
        "If this idea of thinking of a model as a hypothesis is new to you, see this tutorial on the topic:\n",
        "\n",
        "What is a Hypothesis in Machine Learning?\n",
        "Bayes Theorem provides a probabilistic model to describe the relationship between data (D) and a hypothesis (h); for example:\n",
        "\n",
        "$$P(h|D) = P(D|h) * \\frac{P(h)}{P(D)}$$\n",
        "\n",
        "Breaking this down, it says that the probability of a given hypothesis holding or being true given some observed data can be calculated as the probability of observing the data given the hypothesis multiplied by the probability of the hypothesis being true regardless of the data, divided by the probability of observing the data regardless of the hypothesis.\n",
        "\n",
        "*Bayes theorem provides a way to calculate the probability of a hypothesis based on its prior probability, the probabilities of observing various data given the hypothesis, and the observed data itself.*\n",
        "\n",
        "— Page 156, Machine Learning, 1997.\n",
        "\n",
        "Under this framework, each piece of the calculation has a specific name; for example:\n",
        "\n",
        "- P(h|D): Posterior probability of the hypothesis (the thing we want to calculate).\n",
        "- P(h): Prior probability of the hypothesis.\n",
        "\n",
        "This gives a useful framework for thinking about and modeling a machine learning problem.\n",
        "\n",
        "If we have some prior domain knowledge about the hypothesis, this is captured in the prior probability. If we don’t, then all hypotheses may have the same prior probability.\n",
        "\n",
        "If the probability of observing the data P(D) increases, then the probability of the hypothesis holding given the data P(h|D) decreases. Conversely, if the probability of the hypothesis P(h) and the probability of observing the data given hypothesis increases, the probability of the hypothesis holding given the data P(h|D) increases.\n",
        "\n",
        "The notion of testing different models on a dataset in applied machine learning can be thought of as estimating the probability of each hypothesis (h1, h2, h3, … in H) being true given the observed data.\n",
        "\n",
        "The optimization or seeking the hypothesis with the maximum posterior probability in modeling is called maximum a posteriori or MAP for short.\n",
        "\n",
        "*Any such maximally probable hypothesis is called a maximum a posteriori (MAP) hypothesis. We can determine the MAP hypotheses by using Bayes theorem to calculate the posterior probability of each candidate hypothesis.*\n",
        "\n",
        "— Page 157, Machine Learning, 1997.\n",
        "\n",
        "Under this framework, the probability of the data (D) is constant as it is used in the assessment of each hypothesis. Therefore, it can be removed from the calculation to give the simplified unnormalized estimate as follows:\n",
        "\n",
        "max h in H P(h|D) = P(D|h) * P(h)\n",
        "\n",
        "If we do not have any prior information about the hypothesis being tested, they can be assigned a uniform probability, and this term too will be a constant and can be removed from the calculation to give the following:\n",
        "\n",
        "max h in H P(h|D) = P(D|h)\n",
        "\n",
        "That is, the goal is to locate a hypothesis that best explains the observed data.\n",
        "\n",
        "Fitting models like linear regression for predicting a numerical value, and logistic regression for binary classification can be framed and solved under the MAP probabilistic framework. This provides an alternative to the more common maximum likelihood estimation (MLE) framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tw1IQAU-6YI",
        "colab_type": "text"
      },
      "source": [
        "## Bayes Theorem for Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ECS9JI-_uj",
        "colab_type": "text"
      },
      "source": [
        "Classification is a predictive modeling problem that involves assigning a label to a given input data sample.\n",
        "\n",
        "The problem of classification predictive modeling can be framed as calculating the conditional probability of a class label given a data sample, for example:\n",
        "\n",
        "$$P(class|data) = \\frac{P(data|class) * P(class)}{P(data)}$$\n",
        "\n",
        "Where P(class|data) is the probability of class given the provided data.\n",
        "\n",
        "This calculation can be performed for each class in the problem and the class that is assigned the largest probability can be selected and assigned to the input data.\n",
        "\n",
        "In practice, it is very challenging to calculate full Bayes Theorem for classification.\n",
        "\n",
        "The priors for the class and the data are easy to estimate from a training dataset, if the dataset is suitability representative of the broader problem.\n",
        "\n",
        "The conditional probability of the observation based on the class P(data|class) is not feasible unless the number of examples is extraordinarily large, e.g. large enough to effectively estimate the probability distribution for all different possible combinations of values. This is almost never the case, we will not have sufficient coverage of the domain.\n",
        "\n",
        "As such, the direct application of Bayes Theorem also becomes intractable, especially as the number of variables or features (n) increases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_6yymLC-_3M",
        "colab_type": "text"
      },
      "source": [
        "### Naive Bayes Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZNlqRxD-_0J",
        "colab_type": "text"
      },
      "source": [
        "The solution to using Bayes Theorem for a conditional probability classification model is to simplify the calculation.\n",
        "\n",
        "The Bayes Theorem assumes that each input variable is dependent upon all other variables. This is a cause of complexity in the calculation. We can remove this assumption and consider each input variable as being independent from each other.\n",
        "\n",
        "This changes the model from a dependent conditional probability model to an independent conditional probability model and dramatically simplifies the calculation.\n",
        "\n",
        "This means that we calculate P(data|class) for each input variable separately and multiple the results together, for example:\n",
        "\n",
        "$$P(class | X1, X2, …, Xn)$$ = $$\\frac{P(X1|class) * P(X2|class) * … * P(Xn|class) * P(class)}{P(data)}$$\n",
        "\n",
        "We can also drop the probability of observing the data as it is a constant for all calculations, for example:\n",
        "\n",
        "$$P(class | X1, X2, …, Xn) =$$\n",
        "$$P(X1|class) * P(X2|class) * … * P(Xn|class) * P(class)$$\n",
        "\n",
        "This simplification of Bayes Theorem is common and widely used for classification predictive modeling problems and is generally referred to as Naive Bayes.\n",
        "\n",
        "The word “naive” is French and typically has a diaeresis (umlaut) over the “i” (**naïve**), which is commonly left out for simplicity, and “Bayes” is capitalized as it is named for Reverend Thomas Bayes.\n",
        "\n",
        "For tutorials on how to implement Naive Bayes from scratch in Python see:\n",
        "\n",
        "[How to Develop a Naive Bayes Classifier from Scratch in Python](https://machinelearningmastery.com/classification-as-conditional-probability-and-the-naive-bayes-algorithm/)\n",
        "[Naive Bayes Classifier From Scratch in Python](https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHbh7e-7_IhO",
        "colab_type": "text"
      },
      "source": [
        "### Bayes Optimal Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2_DoF1J_Iam",
        "colab_type": "text"
      },
      "source": [
        "The Bayes optimal classifier is a probabilistic model that makes the most likley prediction for a new example, given the training dataset.\n",
        "\n",
        "This model is also referred to as the Bayes optimal learner, the Bayes classifier, Bayes optimal decision boundary, or the Bayes optimal discriminant function.\n",
        "\n",
        "Bayes Classifier: Probabilistic model that makes the most probable prediction for new examples.\n",
        "Specifically, the Bayes optimal classifier answers the question:\n",
        "\n",
        "**What is the most probable classification of the new instance given the training data?**\n",
        "\n",
        "This is different from the MAP (maximum a posteriori)framework that seeks the most probable hypothesis (model). Instead, we are interested in making a specific prediction.\n",
        "\n",
        "The equation below demonstrates how to calculate the conditional probability for a new instance (vi) given the training data (D), given a space of hypotheses (H).\n",
        "\n",
        "$$P(v_j | D) = \\sum_{h\\in H}{P(v_j | h_i) * P(h_i|D)}$$\n",
        "\n",
        "Where vj is a new instance to be classified, H is the set of hypotheses for classifying the instance, hi is a given hypothesis, P(vj | hi) is the posterior probability for vi given hypothesis hi, and P(hi | D) is the posterior probability of the hypothesis hi given the data D.\n",
        "\n",
        "Selecting the outcome with the maximum probability is an example of a Bayes optimal classification.\n",
        "\n",
        "**Any model that classifies examples using this equation is a Bayes optimal classifier and no other model can outperform this technique, on average.**\n",
        "\n",
        "We have to let that sink in. It is a big deal.\n",
        "\n",
        "Because the Bayes classifier is optimal, the Bayes error is the minimum possible error that can be made.\n",
        "\n",
        "**Bayes Error**: The minimum possible error that can be made when making predictions.\n",
        "It is a theoretical model, but it is held up as an ideal that we may wish to pursue.\n",
        "\n",
        "The Naive Bayes classifier is an example of a classifier that adds some simplifying assumptions and attempts to approximate the Bayes Optimal Classifier.\n",
        "\n",
        "For more on the Bayesian optimal classifier, see the tutorial:\n",
        "\n",
        "[A Gentle Introduction to the Bayes Optimal Classifier](https://machinelearningmastery.com/bayes-optimal-classifier/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4ABP10Y_Rlk",
        "colab_type": "text"
      },
      "source": [
        "## More Uses of Bayes Theorem in Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxSD8agS_RcS",
        "colab_type": "text"
      },
      "source": [
        "### Bayesian Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TSWHTUV_RWu",
        "colab_type": "text"
      },
      "source": [
        "Global optimization is a challenging problem of finding an input that results in the minimum or maximum cost of a given objective function.\n",
        "\n",
        "Typically, the form of the objective function is complex and intractable to analyze and is often non-convex, nonlinear, high dimension, noisy, and computationally expensive to evaluate.\n",
        "\n",
        "Bayesian Optimization provides a principled technique based on Bayes Theorem to direct a search of a global optimization problem that is efficient and effective. It works by building a probabilistic model of the objective function, called the surrogate function, that is then searched efficiently with an acquisition function before candidate samples are chosen for evaluation on the real objective function.\n",
        "\n",
        "Bayesian Optimization is often used in applied machine learning to tune the hyperparameters of a given well-performing model on a validation dataset.\n",
        "\n",
        "For more on Bayesian Optimization including how to implement it from scratch, see the tutorial:\n",
        "\n",
        "How to Implement Bayesian Optimization from Scratch in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsYewvzF_XA-",
        "colab_type": "text"
      },
      "source": [
        "### Bayesian Belief Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5xs6PzN_W8p",
        "colab_type": "text"
      },
      "source": [
        "Bayesian Belief Networks\n",
        "Probabilistic models can define relationships between variables and be used to calculate probabilities.\n",
        "\n",
        "Fully conditional models may require an enormous amount of data to cover all possible cases, and probabilities may be intractable to calculate in practice. Simplifying assumptions such as the conditional independence of all random variables can be effective, such as in the case of Naive Bayes, although it is a drastically simplifying step.\n",
        "\n",
        "An alternative is to develop a model that preserves known conditional dependence between random variables and conditional independence in all other cases. Bayesian networks are a probabilistic graphical model that explicitly capture the known conditional dependence with directed edges in a graph model. All missing connections define the conditional independencies in the model.\n",
        "\n",
        "As such Bayesian Networks provide a useful tool to visualize the probabilistic model for a domain, review all of the relationships between the random variables, and reason about causal probabilities for scenarios given available evidence.\n",
        "\n",
        "The networks are not exactly Bayesian by definition, although given that both the probability distributions for the random variables (nodes) and the relationships between the random variables (edges) are specified subjectively, the model can be thought to capture the “belief” about a complex domain.\n",
        "\n",
        "For more on Bayesian Belief Networks, see the tutorial:\n",
        "\n",
        "A Gentle Introduction to Bayesian Belief Networks"
      ]
    }
  ]
}